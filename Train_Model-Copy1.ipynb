{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60978df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from cv2 import resize\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e42ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mild_heart_images = os.listdir(image_directory + 'Mild/')\n",
    "Moderate_heart_images = os.listdir(image_directory + 'Moderate/')\n",
    "No_heart_images = os.listdir(image_directory + 'No/')\n",
    "Proliferate_heart_images = os.listdir(image_directory + 'Proliferate/')\n",
    "Severe_heart_images = os.listdir(image_directory + 'Severe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cadeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(No_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'No/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c46247",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Mild_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Mild/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Moderate_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Moderate/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ca607",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Severe_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Severe/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Proliferate_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Proliferate/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e409eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=np.array(dataset)\n",
    "label=np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, label, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a03ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "    layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),\n",
    "    layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=(244, 244, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed981db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706874e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_augmentation(train_images), train_labels,\n",
    "                    epochs=20,\n",
    "                    validation_data=(data_augmentation(val_images), val_labels),\n",
    "                    callbacks=[reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(data_augmentation(test_images), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy:', test_acc*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15004f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('heart_attack_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = image.astype('float32') / 255.0\n",
    "    return image\n",
    "\n",
    "# Load images and labels\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define complex model architecture\n",
    "inputs = tf.keras.Input(shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "\n",
    "# Convolutional block 1\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Convolutional block 2\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Convolutional block 3\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Convolutional block 4\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Convolutional block 5\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Flatten and fully connected layers\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4096, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(4096, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Increase data augmentation\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=len(train_images) / 32,\n",
    "                    epochs=10,  # Train for more epochs\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "model.save('heart_attack_prediction_model_complex.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = image.astype('float32') / 255.0\n",
    "    return image\n",
    "\n",
    "# Load images and labels\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=len(train_images) / 32,\n",
    "                    epochs=10,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "#model.save('heart_attack_prediction_model_transfer_learning.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a683bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Load images and labels\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = True\n",
    "\n",
    "# Add custom classification head\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    output_layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "#model.save('heart_attack_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Load images and labels\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = True\n",
    "\n",
    "# Add custom classification head\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    output_layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=len(train_images) / 32,\n",
    "                    epochs=50,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "model.save('heart_attack_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1180d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb71233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "image_directory = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bf198",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5952f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = load_data(image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165406ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DenseNet121 model (experiment with different architectures)\n",
    "base_model = DenseNet121(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze some base model layers (adjust number of layers to unfreeze for fine-tuning)\n",
    "base_model.trainable = False  # Freeze all layers by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, select layers to unfreeze for fine-tuning (e.g., base_model.layers[-10:] are the last 10 layers)\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c67f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom classification head\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "dropout = layers.Dropout(0.5)  # Add dropout for regularization\n",
    "output_layer = layers.Dense(5, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e673cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = models.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  dropout,\n",
    "  output_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=15,  # Adjust epochs as needed\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e72dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 58s 735ms/step - loss: 1.3288 - accuracy: 0.5497 - val_loss: 0.9423 - val_accuracy: 0.6672\n",
      "Epoch 2/15\n",
      "74/74 [==============================] - 52s 708ms/step - loss: 0.9823 - accuracy: 0.6487 - val_loss: 0.8143 - val_accuracy: 0.7082\n",
      "Epoch 3/15\n",
      "74/74 [==============================] - 54s 738ms/step - loss: 0.8786 - accuracy: 0.6829 - val_loss: 0.7475 - val_accuracy: 0.7423\n",
      "Epoch 4/15\n",
      "74/74 [==============================] - 55s 747ms/step - loss: 0.8133 - accuracy: 0.7017 - val_loss: 0.7204 - val_accuracy: 0.7645\n",
      "Epoch 5/15\n",
      "74/74 [==============================] - 55s 750ms/step - loss: 0.7684 - accuracy: 0.7157 - val_loss: 0.7344 - val_accuracy: 0.7645\n",
      "Epoch 6/15\n",
      "74/74 [==============================] - 56s 762ms/step - loss: 0.7643 - accuracy: 0.7081 - val_loss: 0.7099 - val_accuracy: 0.7406\n",
      "Epoch 7/15\n",
      "74/74 [==============================] - 55s 744ms/step - loss: 0.7428 - accuracy: 0.7303 - val_loss: 0.6811 - val_accuracy: 0.7543\n",
      "Epoch 8/15\n",
      "74/74 [==============================] - 55s 741ms/step - loss: 0.7146 - accuracy: 0.7350 - val_loss: 0.6532 - val_accuracy: 0.7782\n",
      "Epoch 9/15\n",
      "74/74 [==============================] - 60s 814ms/step - loss: 0.6990 - accuracy: 0.7418 - val_loss: 0.6561 - val_accuracy: 0.7765\n",
      "Epoch 10/15\n",
      "74/74 [==============================] - 55s 752ms/step - loss: 0.7009 - accuracy: 0.7461 - val_loss: 0.6340 - val_accuracy: 0.7867\n",
      "Epoch 11/15\n",
      "74/74 [==============================] - 55s 744ms/step - loss: 0.6704 - accuracy: 0.7525 - val_loss: 0.6622 - val_accuracy: 0.7730\n",
      "Epoch 12/15\n",
      "74/74 [==============================] - 55s 742ms/step - loss: 0.6721 - accuracy: 0.7435 - val_loss: 0.6813 - val_accuracy: 0.7594\n",
      "Epoch 13/15\n",
      "74/74 [==============================] - 55s 744ms/step - loss: 0.6955 - accuracy: 0.7418 - val_loss: 0.6283 - val_accuracy: 0.7833\n",
      "Epoch 14/15\n",
      "74/74 [==============================] - 55s 743ms/step - loss: 0.6695 - accuracy: 0.7490 - val_loss: 0.6940 - val_accuracy: 0.7662\n",
      "Epoch 15/15\n",
      "74/74 [==============================] - 55s 739ms/step - loss: 0.6649 - accuracy: 0.7580 - val_loss: 0.6591 - val_accuracy: 0.7799\n",
      "23/23 [==============================] - 14s 605ms/step - loss: 0.5478 - accuracy: 0.8063\n",
      "Test accuracy: 0.8062756061553955\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07890536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreyas\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('heart_attack_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Load images and labels\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Load pre-trained DenseNet121 model\n",
    "base_model = DenseNet121(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "dropout = layers.Dropout(0.5)  # Add dropout for regularization\n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  dropout,\n",
    "  output_layer\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-5),  # Decreased learning rate\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define a new early stopping callback with increased patience\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=len(train_images) / 32,\n",
    "                    epochs=30,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "# Save the model\n",
    "#model.save('heart_attack_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97788307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('heart_attack_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15e782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
